{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sonakshi1srivastava/ef-prediction-using-random-forest?scriptVersionId=263513074\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nEjection Fraction (EF) Prediction Model - Hackathon Demo\n\nThis script demonstrates a complete machine learning pipeline to predict\nEjection Fraction (EF) using synthetic physiological data.\n\nPipeline Steps:\n1.  Data Generation: Creates a synthetic dataset simulating features from\n    healthy individuals and those with varying degrees of reduced EF.\n    The simulation is based on known physiological correlations:\n    - Reduced EF is often associated with longer QRS duration.\n    - Heart failure can alter heart rate variability (RR intervals).\n    - Reduced physical capacity can be reflected in IMU data (lower activity peaks).\n2.  Model Training: Uses a Random Forest Regressor, a robust ensemble model\n    suitable for this type of regression task.\n3.  Prediction & Evaluation: Splits the data, trains the model, and evaluates\n    its performance on a held-out test set using Mean Absolute Error (MAE)\n    and R-squared (R²) score.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\ndef generate_synthetic_data(n_samples=3000):\n    \"\"\"\n    Generates a synthetic dataset with features correlated to Ejection Fraction.\n    \n    Args:\n        n_samples (int): The total number of samples to generate.\n        \n    Returns:\n        pandas.DataFrame: A DataFrame with features and the target variable 'ef'.\n    \"\"\"\n    print(f\"Generating {n_samples} synthetic data samples...\")\n    np.random.seed(42)\n    \n    # Define categories and their properties\n    categories = {\n        'normal': {'ef_range': (55, 70), 'count': int(n_samples * 0.4),\n                     'rr_mean': 800, 'rr_std': 100,\n                     'qrs_mean': 90, 'qrs_std': 10,\n                     'imu_peak_mean': 1.5, 'imu_peak_std': 0.3},\n        'mild': {'ef_range': (40, 54), 'count': int(n_samples * 0.35),\n                   'rr_mean': 850, 'rr_std': 80,\n                   'qrs_mean': 115, 'qrs_std': 15,\n                   'imu_peak_mean': 1.2, 'imu_peak_std': 0.25},\n        'severe': {'ef_range': (25, 39), 'count': int(n_samples * 0.25),\n                     'rr_mean': 950, 'rr_std': 60,\n                     'qrs_mean': 140, 'qrs_std': 20,\n                     'imu_peak_mean': 0.8, 'imu_peak_std': 0.2}\n    }\n\n    all_data = []\n    for cat, props in categories.items():\n        count = props['count']\n        \n        # Generate correlated features\n        rr = np.random.normal(props['rr_mean'], props['rr_std'], count)\n        qrs = np.random.normal(props['qrs_mean'], props['qrs_std'], count)\n        imu_peak = np.random.normal(props['imu_peak_mean'], props['imu_peak_std'], count)\n        \n        # Generate EF values within the specified range\n        ef = np.random.uniform(props['ef_range'][0], props['ef_range'][1], count)\n        \n        df_cat = pd.DataFrame({\n            'rr_interval_ms': rr,\n            'qrs_duration_ms': qrs,\n            'imu_peak_accel_g': imu_peak,\n            'ef_percentage': ef,\n            'group': cat\n        })\n        all_data.append(df_cat)\n        \n    final_df = pd.concat(all_data).sample(frac=1).reset_index(drop=True)\n    print(\"Synthetic data generation complete.\\n\")\n    return final_df\n\ndef main():\n    \"\"\"Main function to run the ML pipeline.\"\"\"\n    \n    # 1. Data Generation\n    # We generate a synthetic dataset for this demonstration.\n    # In a real project, this would be replaced by loading and cleaning\n    # the CSV file generated by the web UI.\n    df = generate_synthetic_data(n_samples=5000)\n    print(\"Dataset Head:\")\n    print(df.head())\n    print(\"\\nDataset Info:\")\n    df.info()\n    \n    # 2. Feature Engineering & Preparation\n    # The features are already engineered in the data generation step.\n    # We select our features (X) and target (y).\n    features = ['rr_interval_ms', 'qrs_duration_ms', 'imu_peak_accel_g']\n    target = 'ef_percentage'\n    \n    X = df[features]\n    y = df[target]\n    \n    # Split data into training and testing sets (80% train, 20% test)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    print(f\"\\nData split into {len(X_train)} training and {len(X_test)} testing samples.\")\n\n    # 3. Model Training\n    # We use a Random Forest Regressor. n_estimators is the number of trees.\n    # random_state ensures reproducibility.\n    print(\"\\nTraining Random Forest Regressor model...\")\n    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n    model.fit(X_train, y_train)\n    print(\"Model training complete.\")\n\n    # 4. Prediction and Evaluation\n    print(\"\\nEvaluating model performance on the test set...\")\n    y_pred = model.predict(X_test)\n    \n    # Calculate performance metrics\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(\"\\n--- Model Performance Summary ---\")\n    print(f\"Mean Absolute Error (MAE): {mae:.2f}%\")\n    print(f\"R-squared (R²) Score: {r2:.3f}\")\n    print(\"---------------------------------\")\n    print(f\"Interpretation: On average, the model's EF prediction is off by ~{mae:.2f} percentage points.\")\n    print(f\"The model explains {r2*100:.1f}% of the variance in the EF data.\\n\")\n    \n    # Display a few sample predictions for a qualitative check\n    print(\"--- Sample Predictions ---\")\n    results_df = pd.DataFrame({'Actual EF': y_test, 'Predicted EF': y_pred}).reset_index(drop=True)\n    print(results_df.head(10).round(1))\n    print(\"--------------------------\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-23T09:21:24.84075Z","iopub.execute_input":"2025-09-23T09:21:24.84098Z","iopub.status.idle":"2025-09-23T09:21:30.955777Z","shell.execute_reply.started":"2025-09-23T09:21:24.840958Z","shell.execute_reply":"2025-09-23T09:21:30.954374Z"}},"outputs":[{"name":"stdout","text":"Generating 5000 synthetic data samples...\nSynthetic data generation complete.\n\nDataset Head:\n   rr_interval_ms  qrs_duration_ms  imu_peak_accel_g  ef_percentage   group\n0      930.196836       171.953500          0.891009      35.547396  severe\n1      952.285070       104.559443          1.184969      32.705732  severe\n2      705.040853        93.628515          0.894312      48.688917    mild\n3      791.171795        79.291476          1.183191      63.602494  normal\n4     1061.455453       125.168575          0.844478      29.142933  severe\n\nDataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 5 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   rr_interval_ms    5000 non-null   float64\n 1   qrs_duration_ms   5000 non-null   float64\n 2   imu_peak_accel_g  5000 non-null   float64\n 3   ef_percentage     5000 non-null   float64\n 4   group             5000 non-null   object \ndtypes: float64(4), object(1)\nmemory usage: 195.4+ KB\n\nData split into 4000 training and 1000 testing samples.\n\nTraining Random Forest Regressor model...\nModel training complete.\n\nEvaluating model performance on the test set...\n\n--- Model Performance Summary ---\nMean Absolute Error (MAE): 5.15%\nR-squared (R²) Score: 0.708\n---------------------------------\nInterpretation: On average, the model's EF prediction is off by ~5.15 percentage points.\nThe model explains 70.8% of the variance in the EF data.\n\n--- Sample Predictions ---\n   Actual EF  Predicted EF\n0       60.4          66.0\n1       59.0          61.3\n2       64.6          63.0\n3       35.4          38.2\n4       47.5          31.9\n5       48.0          42.5\n6       32.1          44.5\n7       47.1          58.9\n8       28.3          38.4\n9       51.7          45.4\n--------------------------\n\n","output_type":"stream"}],"execution_count":1}]}