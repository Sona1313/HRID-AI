{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# unified_ef_pipeline_pytorch.py\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport math\nimport time\nfrom tqdm import tqdm\n\n# sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, classification_report\n\n# plotting (optional)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# torch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# signal utils\nfrom scipy.signal.windows import gaussian\nfrom scipy.signal import find_peaks\n\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nOUTPUT_DIR = './output_pytorch/'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# --------------------------\n# Cardiac Feature Extractor (same logic)\n# --------------------------\nclass CardiacFeatureExtractor:\n    def __init__(self, sampling_rate=500):\n        self.sampling_rate = sampling_rate\n\n    def detect_r_peaks(self, ecg_signal):\n        try:\n            height_threshold = np.percentile(ecg_signal, 75)\n            distance = int(0.25 * self.sampling_rate)\n            peaks, _ = find_peaks(ecg_signal, height=height_threshold, distance=distance)\n            return peaks\n        except Exception:\n            return np.arange(100, len(ecg_signal)-100, int(0.8 * self.sampling_rate))\n\n    def compute_pep_lvet_ratio(self, ecg, scg_signal):\n        try:\n            r_peaks = self.detect_r_peaks(ecg)\n            if len(r_peaks) < 3:\n                return 80.0, 300.0, 0.27\n            r_times = r_peaks * (1000.0 / self.sampling_rate)\n            scg_peaks, _ = find_peaks(scg_signal, distance=int(0.15 * self.sampling_rate))\n            scg_times = scg_peaks * (1000.0 / self.sampling_rate)\n\n            pep_values = []\n            for r_time in r_times[:5]:\n                subsequent_scg = scg_times[scg_times > r_time]\n                if len(subsequent_scg) > 0:\n                    pep = subsequent_scg[0] - r_time\n                    if 30 <= pep <= 150:\n                        pep_values.append(pep)\n            pep_ms = np.median(pep_values) if pep_values else 80.0\n\n            lvet_values = []\n            for i in range(min(5, len(r_peaks)-1)):\n                r_start = r_times[i]\n                r_end = r_times[i+1]\n                scg_in_beat = scg_times[(scg_times > r_start + 50) & (scg_times < r_end - 50)]\n                if len(scg_in_beat) >= 2:\n                    lvet = scg_in_beat[-1] - scg_in_beat[0]\n                    if 200 <= lvet <= 450:\n                        lvet_values.append(lvet)\n            lvet_ms = np.median(lvet_values) if lvet_values else 300.0\n\n            ratio = pep_ms / lvet_ms if lvet_ms > 0 else 0.27\n            return pep_ms, lvet_ms, ratio\n        except Exception:\n            return 80.0, 300.0, 0.27\n\n# --------------------------\n# Synthetic Data Generator (keeps same behavior)\n# --------------------------\nclass SyntheticDataGenerator:\n    def __init__(self, n_samples=1000, sequence_length=3000, sampling_rate=500):\n        self.n_samples = n_samples\n        self.sequence_length = sequence_length\n        self.sampling_rate = sampling_rate\n        self.feat_ext = CardiacFeatureExtractor(sampling_rate)\n\n    def generate_ecg_signal(self, heart_rate, abnormality_level=0):\n        t = np.linspace(0, 6, self.sequence_length)\n        ecg = np.zeros_like(t)\n        rr_interval = 60.0 / heart_rate\n        rr_variability = 0.1 * abnormality_level if abnormality_level > 0 else 0.02\n        current_time = 0.0\n        while current_time < 6.0:\n            idx = int(current_time * self.sampling_rate)\n            if idx < len(ecg) - 100:\n                qrs_width = int((0.08 + 0.02 * abnormality_level) * self.sampling_rate)\n                qrs = gaussian(qrs_width, std=qrs_width/6) * (1.2 - 0.4 * abnormality_level)\n                end_idx = min(idx + qrs_width, len(ecg))\n                ecg[idx:end_idx] += qrs[:end_idx-idx]\n\n                t_start = idx + int(0.2 * self.sampling_rate)\n                t_width = int(0.15 * self.sampling_rate)\n                if t_start + t_width < len(ecg):\n                    t_wave = gaussian(t_width, std=t_width/4) * (0.3 - 0.15 * abnormality_level)\n                    ecg[t_start:t_start+t_width] += t_wave\n            current_time += rr_interval * (1 + np.random.uniform(-rr_variability, rr_variability))\n        noise_level = 0.02 + 0.03 * abnormality_level\n        ecg += np.random.normal(0, noise_level, len(ecg))\n        return ecg\n\n    def generate_scg_signal(self, r_peaks, pep_ratio, lvet_ratio):\n        scg = np.zeros(self.sequence_length)\n        for r_peak in r_peaks:\n            if r_peak >= self.sequence_length - 200:\n                continue\n            pep_delay = int((0.04 + 0.03 * pep_ratio) * self.sampling_rate)\n            ao_idx = r_peak + pep_delay\n            if ao_idx < self.sequence_length:\n                ao_width = int(0.08 * self.sampling_rate)\n                ao_wave = gaussian(ao_width, std=8) * (0.6 - 0.3 * pep_ratio)\n                end_idx = min(ao_idx + ao_width, self.sequence_length)\n                scg[ao_idx:end_idx] += ao_wave[:end_idx-ao_idx]\n            lvet_delay = int((0.30 - 0.08 * lvet_ratio) * self.sampling_rate)\n            ac_idx = r_peak + lvet_delay\n            if ac_idx < self.sequence_length:\n                ac_width = int(0.06 * self.sampling_rate)\n                ac_wave = gaussian(ac_width, std=6) * (0.4 - 0.2 * lvet_ratio)\n                end_idx = min(ac_idx + ac_width, self.sequence_length)\n                scg[ac_idx:end_idx] += ac_wave[:end_idx-ac_idx]\n        scg += np.random.normal(0, 0.01, len(scg))\n        return scg\n\n    def generate_dataset(self, verbose=True):\n        X_raw = []\n        X_features = []\n        y_ef = []\n        y_category = []\n        summary_data = []\n\n        if verbose:\n            print(f\"Generating {self.n_samples} samples...\")\n\n        for i in range(self.n_samples):\n            category_prob = np.random.random()\n            if category_prob < 0.33:\n                ef = np.random.normal(62, 3)\n                category = \"Normal\"\n                abnormality = 0.0\n                target_pep_ratio = np.random.uniform(0.20, 0.30)\n            elif category_prob < 0.66:\n                ef = np.random.normal(45, 3)\n                category = \"Mildly Reduced\"\n                abnormality = 0.5\n                target_pep_ratio = np.random.uniform(0.30, 0.40)\n            else:\n                ef = np.random.normal(32, 5)\n                category = \"Abnormal\"\n                abnormality = 1.0\n                target_pep_ratio = np.random.uniform(0.40, 0.55)\n\n            ef = max(20, min(80, ef))\n            if category == \"Normal\":\n                hr = np.random.normal(70, 8)\n            elif category == \"Mildly Reduced\":\n                hr = np.random.normal(75, 10)\n            else:\n                hr = np.random.normal(85, 12)\n            hr = max(50, min(120, hr))\n\n            ecg = self.generate_ecg_signal(hr, abnormality)\n            r_peaks = self.feat_ext.detect_r_peaks(ecg)\n            target_lvet_ratio = 1.0 - target_pep_ratio\n            scg = self.generate_scg_signal(r_peaks, target_pep_ratio, target_lvet_ratio)\n\n            t = np.arange(self.sequence_length) / self.sampling_rate\n            hr_hz = hr / 60.0\n            accel_x = np.random.normal(0, 0.03, self.sequence_length) + 0.02 * np.sin(2 * np.pi * hr_hz * t)\n            accel_y = np.random.normal(0, 0.03, self.sequence_length) + 0.015 * np.sin(2 * np.pi * hr_hz * t)\n            accel_z = np.random.normal(0, 0.03, self.sequence_length) + 0.025 * np.sin(2 * np.pi * hr_hz * t)\n\n            accel_x += scg * 0.1\n            accel_y += scg * 0.08\n            accel_z += scg * 0.12\n\n            audio = np.random.normal(0, 0.01, self.sequence_length)\n            for r_peak in r_peaks[:10]:\n                if r_peak + 100 < len(audio):\n                    s1_idx = r_peak + int(0.02 * self.sampling_rate)\n                    s1_dur = int(0.04 * self.sampling_rate)\n                    if s1_idx + s1_dur < len(audio):\n                        audio[s1_idx:s1_idx+s1_dur] += gaussian(s1_dur, std=4) * 0.15\n                    s2_idx = r_peak + int(0.35 * self.sampling_rate)\n                    s2_dur = int(0.03 * self.sampling_rate)\n                    s2_strength = 0.12 - 0.08 * abnormality\n                    if s2_idx + s2_dur < len(audio):\n                        audio[s2_idx:s2_idx+s2_dur] += gaussian(s2_dur, std=3) * s2_strength\n\n            pep_ms, lvet_ms, pep_lvet_ratio = self.feat_ext.compute_pep_lvet_ratio(ecg, scg)\n\n            multi_ch = np.stack([ecg, accel_x, accel_y, accel_z, audio], axis=1).astype(np.float32)\n            X_raw.append(multi_ch)\n\n            features = {\n                'heart_rate': hr,\n                'pep_ms': pep_ms,\n                'lvet_ms': lvet_ms,\n                'pep_lvet_ratio': pep_lvet_ratio,\n                'ecg_mean': float(np.mean(ecg)),\n                'ecg_std': float(np.std(ecg)),\n                'scg_mean': float(np.mean(scg)),\n                'scg_std': float(np.std(scg)),\n            }\n            X_features.append(features)\n            y_ef.append(ef)\n            y_category.append(category)\n            summary_data.append({\n                'sample_id': i,\n                'ejection_fraction': ef,\n                'category': category,\n                'heart_rate': hr,\n                'pep_ms': pep_ms,\n                'lvet_ms': lvet_ms,\n                'pep_lvet_ratio': pep_lvet_ratio,\n                'abnormality_level': abnormality\n            })\n\n            if (i + 1) % 200 == 0 and verbose:\n                print(f\"Generated {i + 1}/{self.n_samples} samples\")\n\n        X_raw = np.array(X_raw)\n        X_df = pd.DataFrame(X_features)\n        y_ef = np.array(y_ef)\n        y_category = np.array(y_category)\n        summary_df = pd.DataFrame(summary_data)\n        return X_raw, X_df, y_ef, y_category, summary_df\n\n# --------------------------\n# PyTorch Dataset\n# --------------------------\nclass CardiacDataset(Dataset):\n    def __init__(self, X, y):\n        # X: numpy (N, seq_len, n_channels)\n        self.X = X.astype(np.float32)\n        self.y = y.astype(np.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x = self.X[idx]  # (seq_len, n_channels)\n        # switch to (channels, seq_len) for conv1d\n        x = np.transpose(x, (1, 0)).astype(np.float32)\n        y = self.y[idx]\n        return torch.from_numpy(x), torch.tensor(y, dtype=torch.float32)\n\n# --------------------------\n# Model (mirrors Keras architecture)\n# --------------------------\nclass EFNet(nn.Module):\n    def __init__(self, in_channels):\n        super(EFNet, self).__init__()\n        # Conv1d expects (batch, channels, seq_len)\n        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=9, padding=4)\n        self.bn1 = nn.BatchNorm1d(32)\n        self.pool1 = nn.MaxPool1d(4)\n\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=7, padding=3)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.pool2 = nn.MaxPool1d(4)\n\n        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.pool3 = nn.MaxPool1d(4)\n\n        self.global_pool = nn.AdaptiveAvgPool1d(1)  # like GlobalAveragePooling1D\n        self.drop1 = nn.Dropout(0.3)\n        self.fc1 = nn.Linear(128, 64)\n        self.drop2 = nn.Dropout(0.2)\n        self.out = nn.Linear(64, 1)\n\n    def forward(self, x):\n        # x: (batch, channels, seq_len)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool1(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool2(x)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.pool3(x)\n        x = self.global_pool(x)  # (batch, 128, 1)\n        x = x.view(x.size(0), -1)  # (batch, 128)\n        x = self.drop1(x)\n        x = F.relu(self.fc1(x))\n        x = self.drop2(x)\n        x = self.out(x)\n        return x.squeeze(1)\n\n# --------------------------\n# Trainer (simple training loop with early stopping)\n# --------------------------\nclass Trainer:\n    def __init__(self, model, device=DEVICE):\n        self.model = model.to(device)\n        self.device = device\n\n    def fit(self, train_loader, val_loader, epochs=50, lr=1e-3, checkpoint_path=None, patience=10):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n        best_val = math.inf\n        best_epoch = 0\n        history = {'train_loss': [], 'val_loss': []}\n\n        for epoch in range(1, epochs+1):\n            self.model.train()\n            train_losses = []\n            for xb, yb in train_loader:\n                xb = xb.to(self.device)\n                yb = yb.to(self.device)\n                preds = self.model(xb)\n                loss = criterion(preds, yb)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                train_losses.append(loss.item())\n\n            val_losses = []\n            self.model.eval()\n            with torch.no_grad():\n                for xb, yb in val_loader:\n                    xb = xb.to(self.device)\n                    yb = yb.to(self.device)\n                    preds = self.model(xb)\n                    loss = criterion(preds, yb)\n                    val_losses.append(loss.item())\n\n            avg_train = np.mean(train_losses)\n            avg_val = np.mean(val_losses)\n            history['train_loss'].append(avg_train)\n            history['val_loss'].append(avg_val)\n\n            print(f\"Epoch {epoch}/{epochs} - train_loss: {avg_train:.4f} - val_loss: {avg_val:.4f}\")\n\n            if avg_val < best_val - 1e-6:\n                best_val = avg_val\n                best_epoch = epoch\n                if checkpoint_path:\n                    torch.save(self.model.state_dict(), checkpoint_path)\n                print(f\"  New best val {best_val:.5f} saved.\")\n            if epoch - best_epoch >= patience:\n                print(f\"Early stopping at epoch {epoch}. Best epoch was {best_epoch}.\")\n                break\n        # load best\n        if checkpoint_path and os.path.exists(checkpoint_path):\n            self.model.load_state_dict(torch.load(checkpoint_path, map_location=self.device))\n        return history\n\n# --------------------------\n# Helper: EF -> category\n# --------------------------\ndef ef_to_category(ef):\n    if ef >= 50:\n        return \"Normal\"\n    elif ef >= 41:\n        return \"Mildly Reduced\"\n    else:\n        return \"Abnormal\"\n\n# --------------------------\n# Inference function (user requested)\n# --------------------------\ndef infer_ef(model_path, scalers_path, ef_scaler_path, sample, device=None):\n    \"\"\"\n    Robust inference helper.\n\n    Args:\n        model_path: path to saved model state_dict (.pth/.pt)\n        scalers_path: joblib file containing dict of per-channel StandardScaler objects\n        ef_scaler_path: joblib file for the EF target scaler\n        sample: numpy array of shape\n                - (seq_len, n_channels)  OR\n                - (n_channels, seq_len)  OR\n                - (batch, seq_len, n_channels)\n        device: torch device string or None (auto)\n    Returns:\n        dict with keys:\n            'ef_pred': float or list of floats\n            'category': str or list of strs\n    \"\"\"\n    # choose device\n    device = DEVICE if device is None else torch.device(device)\n\n    # load scalers\n    scalers = joblib.load(scalers_path)    # dict like {'channel_0': scaler, ...}\n    ef_scaler = joblib.load(ef_scaler_path)\n\n    # determine expected n_channels\n    n_channels_expected = len(scalers)\n\n    arr = np.array(sample, dtype=np.float32)\n\n    # Normalize shape handling:\n    # Accept shapes: (seq_len, n_channels), (n_channels, seq_len), (batch, seq_len, n_channels)\n    if arr.ndim == 2:\n        # ambiguous: either (seq_len, n_channels) or (n_channels, seq_len)\n        if arr.shape[1] == n_channels_expected:\n            # (seq_len, n_channels) -> good\n            arr = arr[np.newaxis, ...]  # make batch\n        elif arr.shape[0] == n_channels_expected:\n            # (n_channels, seq_len) -> transpose to (seq_len, n_channels)\n            arr = arr.T[np.newaxis, ...]\n        else:\n            raise ValueError(f\"Cannot infer channels. Got shape {arr.shape} but scalers expect {n_channels_expected} channels.\")\n    elif arr.ndim == 3:\n        # assume (batch, seq_len, n_channels) or (batch, n_channels, seq_len)\n        if arr.shape[2] == n_channels_expected:\n            # good: (batch, seq_len, n_channels)\n            pass\n        elif arr.shape[1] == n_channels_expected:\n            # (batch, n_channels, seq_len) -> transpose each sample\n            arr = np.transpose(arr, (0, 2, 1))\n        else:\n            raise ValueError(f\"Cannot infer channels from batch. Got shape {arr.shape} but scalers expect {n_channels_expected} channels.\")\n    else:\n        raise ValueError(\"sample must be 2D or 3D numpy array\")\n\n    # Now arr is (batch, seq_len, n_channels)\n    batch_size, seq_len, n_channels = arr.shape\n\n    # Apply per-channel scalers (they were fit on flattened values)\n    X_scaled = np.zeros_like(arr)\n    for ch in range(n_channels):\n        scaler = scalers[f'channel_{ch}']\n        flat = arr[:, :, ch].reshape(-1, 1)\n        transformed = scaler.transform(flat).reshape(batch_size, seq_len)\n        X_scaled[:, :, ch] = transformed\n\n    # Convert to (batch, channels, seq_len) for Conv1d\n    X_tensor = torch.from_numpy(np.transpose(X_scaled, (0, 2, 1))).to(device).float()\n\n    # Build model architecture matching training and load state_dict\n    in_channels = X_tensor.shape[1]\n    model = EFNet(in_channels).to(device)\n    state = torch.load(model_path, map_location=device)\n\n    # If user saved a full checkpoint dict vs state_dict, handle both:\n    if isinstance(state, dict) and ('model_state_dict' in state or 'state_dict' in state):\n        # common patterns: {'model_state_dict':..., ...} or {'state_dict':...}\n        key = 'model_state_dict' if 'model_state_dict' in state else 'state_dict'\n        model.load_state_dict(state[key])\n    else:\n        # assume raw state_dict\n        model.load_state_dict(state)\n\n    model.eval()\n    with torch.no_grad():\n        preds_scaled = model(X_tensor).cpu().numpy().reshape(-1, 1)\n\n    # inverse transform using ef_scaler if possible\n    try:\n        preds_unscaled = ef_scaler.inverse_transform(preds_scaled).flatten()\n    except Exception:\n        preds_unscaled = preds_scaled.flatten()\n\n    # return single or batch friendly\n    if len(preds_unscaled) == 1:\n        val = float(preds_unscaled[0])\n        return {'ef_pred': val, 'category': ef_to_category(val)}\n    else:\n        vals = preds_unscaled.tolist()\n        cats = [ef_to_category(v) for v in vals]\n        return {'ef_pred': vals, 'category': cats}\n\n\n# --------------------------\n# Full main (generate -> train -> eval)\n# --------------------------\ndef main():\n    print(\"=== CARDIAC EF PREDICTION PIPELINE (PyTorch) ===\")\n    # Config\n    n_samples = 1000\n    seq_len = 3000\n    sampling_rate = 500\n    batch_size = 16\n    epochs = 50\n    patience = 8\n    lr = 1e-3\n\n    # Generate\n    generator = SyntheticDataGenerator(n_samples=n_samples, sequence_length=seq_len, sampling_rate=sampling_rate)\n    X_raw, X_df, y_ef, y_category, summary_df = generator.generate_dataset(verbose=True)\n\n    print(f\"Generated dataset shapes: X_raw {X_raw.shape}, y_ef {y_ef.shape}\")\n    summary_df.to_csv(os.path.join(OUTPUT_DIR, 'cardiac_dataset_summary.csv'), index=False)\n\n    # map categories for stratify (just like original)\n    category_map = {\"Normal\": 0, \"Mildly Reduced\": 1, \"Abnormal\": 2}\n    y_cat_num = np.array([category_map[c] for c in y_category])\n\n    # split\n    X_train, X_test, y_train, y_test, y_cat_train, y_cat_test = train_test_split(\n        X_raw, y_ef, y_cat_num, test_size=0.2, random_state=SEED, stratify=y_cat_num\n    )\n\n    # scale per-channel\n    n_channels = X_train.shape[2]\n    scalers = {}\n    for ch in range(n_channels):\n        scaler = StandardScaler()\n        X_train_flat = X_train[:, :, ch].reshape(-1, 1)\n        X_test_flat = X_test[:, :, ch].reshape(-1, 1)\n        scaler.fit(X_train_flat)\n        # transform and assign back\n        X_train[:, :, ch] = scaler.transform(X_train_flat).reshape(X_train.shape[0], -1)\n        X_test[:, :, ch] = scaler.transform(X_test_flat).reshape(X_test.shape[0], -1)\n        scalers[f'channel_{ch}'] = scaler\n\n    # scale EF target\n    ef_scaler = StandardScaler()\n    y_train_scaled = ef_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n    y_test_scaled = ef_scaler.transform(y_test.reshape(-1, 1)).flatten()\n\n    # Save scalers\n    scalers_path = os.path.join(OUTPUT_DIR, 'channel_scalers.joblib')\n    ef_scaler_path = os.path.join(OUTPUT_DIR, 'ef_scaler.joblib')\n    joblib.dump(scalers, scalers_path)\n    joblib.dump(ef_scaler, ef_scaler_path)\n    print(f\"Saved scalers to {scalers_path} and {ef_scaler_path}\")\n\n    # Datasets & loaders (we'll use scaled targets)\n    train_ds = CardiacDataset(X_train, y_train_scaled)\n    val_ds = CardiacDataset(X_test, y_test_scaled)  # using test as val for simplicity\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n\n    # model\n    model = EFNet(in_channels=n_channels)\n    trainer = Trainer(model, device=DEVICE)\n\n    checkpoint_path = os.path.join(OUTPUT_DIR, 'best_model.pth')\n    history = trainer.fit(train_loader, val_loader, epochs=epochs, lr=lr, checkpoint_path=checkpoint_path, patience=patience)\n\n    # Evaluate on test (unscale predictions)\n    # load best model\n    model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n    model.to(DEVICE)\n    model.eval()\n    preds_scaled = []\n    y_true = []\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb = xb.to(DEVICE)\n            out = model(xb).cpu().numpy()\n            preds_scaled.extend(out.tolist())\n            y_true.extend(yb.cpu().numpy().tolist())\n\n    preds_scaled = np.array(preds_scaled).reshape(-1, 1)\n    preds = ef_scaler.inverse_transform(preds_scaled).flatten()\n    y_true_unscaled = ef_scaler.inverse_transform(np.array(y_true).reshape(-1, 1)).flatten()\n\n    # metrics\n    mae = mean_absolute_error(y_true_unscaled, preds)\n    rmse = math.sqrt(mean_squared_error(y_true_unscaled, preds))\n    r2 = r2_score(y_true_unscaled, preds)\n    print(f\"MAE: {mae:.3f}, RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n\n    # categories\n    y_true_cat_labels = [ef_to_category(v) for v in y_true_unscaled]\n    y_pred_cat_labels = [ef_to_category(v) for v in preds]\n\n    print(\"Classification report:\")\n    print(classification_report(y_true_cat_labels, y_pred_cat_labels, target_names=[\"Normal\", \"Mildly Reduced\", \"Abnormal\"]))\n\n    # Save final model (state_dict)\n    final_model_path = os.path.join(OUTPUT_DIR, 'final_cardiac_ef_model.pth')\n    torch.save(model.state_dict(), final_model_path)\n    print(f\"Saved final model to {final_model_path}\")\n\n    # Save results CSV\n    results_df = pd.DataFrame({\n        'metric': ['MAE', 'RMSE', 'R2'],\n        'value': [mae, rmse, r2]\n    })\n    results_df.to_csv(os.path.join(OUTPUT_DIR, 'final_results.csv'), index=False)\n    print(f\"Saved results to {os.path.join(OUTPUT_DIR, 'final_results.csv')}\")\n\n    # Example inference usage\n    example_sample = X_test[0]  # NOTE: already scaled -- infer_ef expects raw sample (unscaled)\n    # But we saved scalers and final model, so pass raw (unscaled) sample from original X_raw to infer_ef\n    raw_sample = X_raw[0]  # (seq_len, n_channels), raw unscaled\n    infer_res = infer_ef(final_model_path, scalers_path, ef_scaler_path, raw_sample, device=str(DEVICE))\n    print(\"Example inference result:\", infer_res)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T12:28:02.192215Z","iopub.execute_input":"2025-10-04T12:28:02.192571Z","iopub.status.idle":"2025-10-04T12:28:53.227233Z","shell.execute_reply.started":"2025-10-04T12:28:02.192544Z","shell.execute_reply":"2025-10-04T12:28:53.226207Z"}},"outputs":[{"name":"stdout","text":"=== CARDIAC EF PREDICTION PIPELINE (PyTorch) ===\nGenerating 1000 samples...\nGenerated 200/1000 samples\nGenerated 400/1000 samples\nGenerated 600/1000 samples\nGenerated 800/1000 samples\nGenerated 1000/1000 samples\nGenerated dataset shapes: X_raw (1000, 3000, 5), y_ef (1000,)\nSaved scalers to ./output_pytorch/channel_scalers.joblib and ./output_pytorch/ef_scaler.joblib\nEpoch 1/50 - train_loss: 0.5317 - val_loss: 0.6814\n  New best val 0.68139 saved.\nEpoch 2/50 - train_loss: 0.2267 - val_loss: 0.4951\n  New best val 0.49515 saved.\nEpoch 3/50 - train_loss: 0.1871 - val_loss: 0.1510\n  New best val 0.15104 saved.\nEpoch 4/50 - train_loss: 0.1902 - val_loss: 0.1679\nEpoch 5/50 - train_loss: 0.1788 - val_loss: 0.1029\n  New best val 0.10288 saved.\nEpoch 6/50 - train_loss: 0.1699 - val_loss: 0.4949\nEpoch 7/50 - train_loss: 0.1736 - val_loss: 0.1353\nEpoch 8/50 - train_loss: 0.1894 - val_loss: 0.0940\n  New best val 0.09402 saved.\nEpoch 9/50 - train_loss: 0.1315 - val_loss: 0.1630\nEpoch 10/50 - train_loss: 0.1506 - val_loss: 0.1036\nEpoch 11/50 - train_loss: 0.1781 - val_loss: 0.5660\nEpoch 12/50 - train_loss: 0.1602 - val_loss: 0.6293\nEpoch 13/50 - train_loss: 0.1736 - val_loss: 0.1124\nEpoch 14/50 - train_loss: 0.1418 - val_loss: 0.4670\nEpoch 15/50 - train_loss: 0.1093 - val_loss: 0.1166\nEpoch 16/50 - train_loss: 0.1293 - val_loss: 0.7962\nEarly stopping at epoch 16. Best epoch was 8.\nMAE: 3.082, RMSE: 3.973, R2: 0.906\nClassification report:\n                precision    recall  f1-score   support\n\n        Normal       0.91      0.92      0.91        75\nMildly Reduced       0.81      0.87      0.84        54\n      Abnormal       1.00      0.93      0.96        71\n\n      accuracy                           0.91       200\n     macro avg       0.91      0.91      0.91       200\n  weighted avg       0.91      0.91      0.91       200\n\nSaved final model to ./output_pytorch/final_cardiac_ef_model.pth\nSaved results to ./output_pytorch/final_results.csv\nExample inference result: {'ef_pred': 42.9360237121582, 'category': 'Mildly Reduced'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}